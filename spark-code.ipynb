{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6d589b",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "This notebook demonstrates an Apache Spark pipeline for investigating flight delay patterns in US aviation data. The workflow encompasses data preprocessing, feature engineering, exploratory analysis through grouped aggregations, and predictive modeling using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96357da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73589196",
   "metadata": {},
   "source": [
    "## Environment & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ef17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create dirs for raw data and partitioned data (Parquet)\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "os.makedirs(\"data/lake\", exist_ok=True)\n",
    "\n",
    "# Move CSVs to \"raw\" data dir\n",
    "shutil.move(\"flights.csv\", \"data/raw/flights.csv\")\n",
    "shutil.move(\"airlines.csv\", \"data/raw/airlines.csv\")\n",
    "shutil.move(\"airports.csv\", \"data/raw/airports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get Spark Session\n",
    "spark = (SparkSession.builder\n",
    "        .appName(\"Flights-Delay\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564e82a",
   "metadata": {},
   "source": [
    "## Schema & Data Ingestion\n",
    "\n",
    "Data is read using explicit **schemas** (instead of inference) to ensure robust typing and data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d4501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "# Explicit Flights Schema\n",
    "flight_schema = T.StructType([\n",
    "    T.StructField(\"YEAR\", T.IntegerType(), True),\n",
    "    T.StructField(\"MONTH\", T.IntegerType(), True),\n",
    "    T.StructField(\"DAY\", T.IntegerType(), True),\n",
    "    T.StructField(\"DAY_OF_WEEK\", T.IntegerType(), True),\n",
    "    T.StructField(\"AIRLINE\", T.StringType(),  True),\n",
    "    T.StructField(\"FLIGHT_NUMBER\", T.StringType(),  True),\n",
    "    T.StructField(\"TAIL_NUMBER\", T.StringType(),  True),\n",
    "    T.StructField(\"ORIGIN_AIRPORT\", T.StringType(),  True),\n",
    "    T.StructField(\"DESTINATION_AIRPORT\", T.StringType(),  True),\n",
    "    T.StructField(\"SCHEDULED_DEPARTURE\", T.IntegerType(), True),\n",
    "    T.StructField(\"DEPARTURE_TIME\", T.IntegerType(), True),\n",
    "    T.StructField(\"DEPARTURE_DELAY\", T.DoubleType(),  True),\n",
    "    T.StructField(\"TAXI_OUT\", T.DoubleType(),  True),\n",
    "    T.StructField(\"WHEELS_OFF\", T.IntegerType(), True),\n",
    "    T.StructField(\"SCHEDULED_TIME\", T.DoubleType(),  True),\n",
    "    T.StructField(\"ELAPSED_TIME\", T.DoubleType(),  True),\n",
    "    T.StructField(\"AIR_TIME\", T.DoubleType(),  True),\n",
    "    T.StructField(\"DISTANCE\", T.DoubleType(),  True),\n",
    "    T.StructField(\"WHEELS_ON\", T.IntegerType(), True),\n",
    "    T.StructField(\"TAXI_IN\", T.DoubleType(),  True),\n",
    "    T.StructField(\"SCHEDULED_ARRIVAL\", T.IntegerType(), True),\n",
    "    T.StructField(\"ARRIVAL_TIME\", T.IntegerType(), True),\n",
    "    T.StructField(\"ARRIVAL_DELAY\", T.DoubleType(),  True),\n",
    "    T.StructField(\"DIVERTED\", T.IntegerType(), True),\n",
    "    T.StructField(\"CANCELLED\", T.IntegerType(), True),\n",
    "    T.StructField(\"CANCELLATION_REASON\", T.StringType(),  True),\n",
    "    T.StructField(\"AIR_SYSTEM_DELAY\", T.DoubleType(),  True),\n",
    "    T.StructField(\"SECURITY_DELAY\", T.DoubleType(),  True),\n",
    "    T.StructField(\"AIRLINE_DELAY\", T.DoubleType(),  True),\n",
    "    T.StructField(\"LATE_AIRCRAFT_DELAY\", T.DoubleType(),  True),\n",
    "    T.StructField(\"WEATHER_DELAY\", T.DoubleType(),  True),\n",
    "])\n",
    "\n",
    "fl_raw = (spark.read.option(\"header\", True).schema(flight_schema)\n",
    "          .csv(\"data/raw/flights.csv\"))\n",
    "\n",
    "fl_raw.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddecf348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit Airlines Schema\n",
    "airline_schema = T.StructType([\n",
    "    T.StructField(\"IATA_CODE\", T.StringType(), True),\n",
    "    T.StructField(\"AIRLINE\",   T.StringType(), True),\n",
    "])\n",
    "\n",
    "airlines = (spark.read\n",
    "            .option(\"header\", True)\n",
    "            .schema(airline_schema)\n",
    "            .csv(\"data/raw/airlines.csv\"))\n",
    "\n",
    "airlines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07864209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit Airports Schema\n",
    "airport_schema = T.StructType([\n",
    "    T.StructField(\"IATA_CODE\", T.StringType(), True),\n",
    "    T.StructField(\"AIRPORT\",   T.StringType(), True),\n",
    "    T.StructField(\"CITY\",      T.StringType(), True),\n",
    "    T.StructField(\"STATE\",     T.StringType(), True),\n",
    "    T.StructField(\"COUNTRY\",   T.StringType(), True),\n",
    "    T.StructField(\"LATITUDE\",  T.DoubleType(), True),\n",
    "    T.StructField(\"LONGITUDE\", T.DoubleType(), True),\n",
    "])\n",
    "\n",
    "airports = (spark.read\n",
    "            .option(\"header\", True)\n",
    "            .schema(airport_schema)\n",
    "            .csv(\"data/raw/airports.csv\"))\n",
    "\n",
    "airports.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307974d2",
   "metadata": {},
   "source": [
    "## Data Preparation and Parquet Export\n",
    "\n",
    "Creates a proper date column from separate *year/month/day* fields and saves all datasets in efficient `Parquet` format. Flight data is partitioned by `YEAR` and `MONTH` for optimal query performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "fl = fl_raw.withColumn(\n",
    "    \"FL_DATE\",\n",
    "    F.to_date(F.format_string(\"%04d-%02d-%02d\", F.col(\"YEAR\"), F.col(\"MONTH\"), F.col(\"DAY\")))\n",
    ")\n",
    "\n",
    "\n",
    "(fl.write.mode(\"overwrite\")\n",
    "   .partitionBy(\"YEAR\",\"MONTH\")\n",
    "   .parquet(\"data/lake/flights_parquet\"))\n",
    "\n",
    "airlines.write.mode(\"overwrite\").parquet(\"data/lake/airlines_parquet\")\n",
    "airports.write.mode(\"overwrite\").parquet(\"data/lake/airports_parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c81f8",
   "metadata": {},
   "source": [
    "## Read Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4497af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy Parquet Reading\n",
    "flights = spark.read.parquet(\"data/lake/flights_parquet\")\n",
    "airlines = spark.read.parquet(\"data/lake/airlines_parquet\")\n",
    "airports = spark.read.parquet(\"data/lake/airports_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46093a8",
   "metadata": {},
   "source": [
    "# Transformations and UDFs\n",
    "\n",
    "Native Spark functions are primarily used to derive evaluation characteristics because they are considerably faster than user-defined functions (UDFs). UDFs are only used where native functions are insufficient or where special business logic is required.\n",
    "\n",
    "The following code illustrates the use of a combination of native functions for standard transformations and UDFs for more complex transformations, such as mapping cancellation codes to descriptive labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=T.IntegerType())\n",
    "def hhmm_to_hour(x):\n",
    "    \"\"\"\n",
    "    Extract hour from time in HHMM format.\n",
    "    \n",
    "    Args:\n",
    "        x: Time in HHMM format (e.g., 1430 for 14:30)\n",
    "        \n",
    "    Returns:\n",
    "        Integer hour (0-23) or None for invalid input.\n",
    "        Example: 1430 -> 14, 0630 -> 6, 2400 -> 0\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    try:\n",
    "        v = int(x)\n",
    "        if v == 2400: # Handle midnight (2400 -> 0)\n",
    "            return 0\n",
    "        if v < 0: # Reject negative values\n",
    "            return None\n",
    "        hh = v // 100 # Extract hour (first 2 digits)\n",
    "        mm = v % 100 # Extract minutes (last 2 digits)\n",
    "        return hh if 0 <= hh <= 23 and 0 <= mm <= 59 else None\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4720845",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping = {\"A\": \"Air Carrier\", \"B\": \"Extreme Weather\", \n",
    "            \"C\": \"National Aviation System\", \"D\": \"Security\"}\n",
    "\n",
    "@F.udf(T.StringType())\n",
    "def cancel_code_to_label(c):\n",
    "    \"\"\"\n",
    "    Convert cancellation code to human-readable description.\n",
    "    \n",
    "    Args:\n",
    "        c: Single letter cancellation code (A, B, C, D)\n",
    "        \n",
    "    Returns:\n",
    "        String description of cancellation reason\n",
    "    \"\"\"\n",
    "    if c is None:\n",
    "        return \"Not Canceled\"\n",
    "    return code_mapping.get(str(c).strip().upper(), \"Other/Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply custom UDFs for complex transformations (time parsing, code mapping) and native Spark functions for simple operations (comparisons, date formatting)\n",
    "flights_transformed = (flights\n",
    "    .withColumn(\"DEP_HOUR\", hhmm_to_hour(\"SCHEDULED_DEPARTURE\")) # UDF\n",
    "    .withColumn(\"IS_DELAYED_15\", (F.col(\"ARRIVAL_DELAY\") >= 15).cast(\"int\")) # native\n",
    "    .withColumn(\"DOW\", F.date_format(\"FL_DATE\", \"E\")) # native\n",
    "    .withColumn(\"CANCEL_REASON_LABEL\", cancel_code_to_label(\"CANCELLATION_REASON\")) # UDF\n",
    ")\n",
    "\n",
    "flights_transformed.select(\"FL_DATE\",\"DOW\",\"DAY_OF_WEEK\", \"DEP_HOUR\", \"SCHEDULED_DEPARTURE\", \"IS_DELAYED_15\", \"CANCEL_REASON_LABEL\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10623f3b",
   "metadata": {},
   "source": [
    "## Data Enrichment with Reference Tables\n",
    "\n",
    "Enriches the flight dataset by joining with airline and airport reference data to replace short codes with human-readable values.\n",
    "\n",
    "**Broadcast Join Optimization:** Small reference tables (airlines, airports) are broadcasted to all worker nodes to avoid expensive shuffle operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8065fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare airlines reference table \n",
    "alN = airlines.withColumnRenamed(\"AIRLINE\", \"CARRIER_NAME\")\n",
    "\n",
    "# Join flight data with airline info using broadcast join for performance\n",
    "flights_airlines_enriched = flights_transformed.join(F.broadcast(alN), flights_transformed.AIRLINE == alN.IATA_CODE, \"left\")\n",
    "\n",
    "flights_airlines_enriched.select(\"CARRIER_NAME\", \"AIRLINE\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c074ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create origin airports view\n",
    "airports_origin = airports.select(F.col(\"IATA_CODE\").alias(\"ORIGIN_CODE\"),\n",
    "                F.col(\"AIRPORT\").alias(\"ORIGIN_AIRPORT_NAME\"),\n",
    "                F.col(\"STATE\").alias(\"ORIGIN_STATE\"))\n",
    "\n",
    "# Create destination airports view\n",
    "airports_dest = airports.select(F.col(\"IATA_CODE\").alias(\"DEST_CODE\"),\n",
    "                F.col(\"AIRPORT\").alias(\"DEST_AIRPORT_NAME\"),\n",
    "                F.col(\"STATE\").alias(\"DEST_STATE\"))\n",
    "\n",
    "# Join\n",
    "flights_enriched = (flights_airlines_enriched\n",
    "  .join(F.broadcast(airports_origin), flights_airlines_enriched.ORIGIN_AIRPORT == F.col(\"ORIGIN_CODE\"), \"left\")\n",
    "  .join(F.broadcast(airports_dest), flights_airlines_enriched.DESTINATION_AIRPORT == F.col(\"DEST_CODE\"), \"left\"))\n",
    "\n",
    "flights_enriched.select(\"CARRIER_NAME\", \"ORIGIN_AIRPORT_NAME\", \"DEST_AIRPORT_NAME\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55192ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enriched data partitioned by YEAR and MONTH\n",
    "(flights_enriched.write.mode(\"overwrite\")\n",
    "   .partitionBy(\"YEAR\",\"MONTH\")\n",
    "   .parquet(\"data/lake/enriched_parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8022af",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis through Filtering and Aggregation\n",
    "\n",
    "This section demonstrates Spark's distributed aggregation capabilities through comprehensive flight delay analysis. We filter the dataset to focus on completed flights (no cancellations or diversions) and perform grouped aggregations across multiple dimensions to identify delay patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enriched dataset and filter for completed flights only\n",
    "df = spark.read.parquet(\"data/lake/enriched_parquet\")\n",
    "df = df.filter((F.col(\"CANCELLED\")==0) & (F.col(\"DIVERTED\")==0))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_airline = (df.groupBy(\"AIRLINE\",\"CARRIER_NAME\")\n",
    "  .agg(F.count(\"*\").alias(\"flights\"),\n",
    "       F.avg(\"IS_DELAYED_15\").alias(\"p_delay\"),\n",
    "       F.avg(\"ARRIVAL_DELAY\").alias(\"avg_arr_delay\"))\n",
    "  .filter(\"flights >= 500\")\n",
    "  .orderBy(F.desc(\"p_delay\")))\n",
    "\n",
    "print(f\"Input partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "by_airline.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c056c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed execution plan for airline aggregation query\n",
    "by_airline.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a005ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_origin = (df.groupBy(\"ORIGIN_AIRPORT\",\"ORIGIN_AIRPORT_NAME\",\"ORIGIN_STATE\")\n",
    "  .agg(F.count(\"*\").alias(\"n\"),\n",
    "       F.avg(\"IS_DELAYED_15\").alias(\"p_delay\"))\n",
    "  .filter(\"n >= 300\")\n",
    "  .orderBy(F.desc(\"p_delay\")))\n",
    "\n",
    "by_origin.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53090001",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_route = (df.groupBy(\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\")\n",
    "  .agg(F.count(\"*\").alias(\"n\"),\n",
    "       F.avg(\"IS_DELAYED_15\").alias(\"p_delay\"),\n",
    "       F.avg(\"ARRIVAL_DELAY\").alias(\"avg_arr_delay\"))\n",
    "  .filter(\"n >= 200\")\n",
    "  .orderBy(F.desc(\"p_delay\")))\n",
    "\n",
    "by_route.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_hour = (df.groupBy(\"DEP_HOUR\")\n",
    "  .agg(F.count(\"*\").alias(\"n\"),\n",
    "       F.avg(\"IS_DELAYED_15\").alias(\"p_delay\"))\n",
    "  .orderBy(\"DEP_HOUR\"))\n",
    "\n",
    "by_hour.show(24, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9157c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_dow = (df.groupBy(\"DOW\")\n",
    "  .agg(F.count(\"*\").alias(\"n\"),\n",
    "       F.avg(\"IS_DELAYED_15\").alias(\"p_delay\"))\n",
    "  .orderBy(\"DOW\"))\n",
    "\n",
    "by_dow.show(7, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8071e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create seasonal categories\n",
    "season = (F.when(F.col(\"MONTH\").isin(12,1,2), \"winter\")\n",
    "            .when(F.col(\"MONTH\").isin(3,4,5), \"spring\")\n",
    "            .when(F.col(\"MONTH\").isin(6,7,8), \"summer\")\n",
    "            .otherwise(\"autumn\"))\n",
    "df = df.withColumn(\"SEASON\", season)\n",
    "\n",
    "by_season = (df.groupBy(\"SEASON\")\n",
    "  .agg(F.count(\"*\").alias(\"n\"),\n",
    "       F.avg(\"IS_DELAYED_15\").alias(\"p_delay\"),\n",
    "       F.avg(\"ARRIVAL_DELAY\").alias(\"avg_arr_delay\"))\n",
    "  .orderBy(\"SEASON\"))\n",
    "\n",
    "by_season.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distance categories\n",
    "dist_bins = (F.when(F.col(\"DISTANCE\") < 500, \"short\")\n",
    "               .when(F.col(\"DISTANCE\") < 1500, \"medium\")\n",
    "               .otherwise(\"long\"))\n",
    "df = df.withColumn(\"DIST_BIN\", dist_bins)\n",
    "\n",
    "\n",
    "by_dist = (df.groupBy(\"DIST_BIN\")\n",
    "  .agg(F.count(\"*\").alias(\"n\"),\n",
    "       F.avg(\"IS_DELAYED_15\").alias(\"p_delay\"),\n",
    "       F.avg(\"ARRIVAL_DELAY\").alias(\"avg_arr_delay\"))\n",
    "  .orderBy(\"DIST_BIN\"))\n",
    "\n",
    "by_dist.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873a794",
   "metadata": {},
   "source": [
    "## Spark ML\n",
    "\n",
    "A complete machine learning pipeline is implemented in Apache Spark to predict flight delays. The aim is to use a logistic regression for binary classification to predict whether a flight will be delayed by at least 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Training Features\n",
    "ml_data = (df\n",
    "    .withColumn(\"label\", F.col(\"IS_DELAYED_15\").cast(\"double\"))  # Create Label Col (delayed = 1.0, on-time = 0.0)\n",
    "    .select(\"label\", \"DEP_HOUR\", \"MONTH\", \"DISTANCE\",\n",
    "            \"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\")\n",
    "    .na.drop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb119cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Categorical Features\n",
    "categorical_columns = [\"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "\n",
    "# Convert Categorical Features\n",
    "string_indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=f\"{column}_indexed\", handleInvalid=\"keep\") \n",
    "    for column in categorical_columns\n",
    "]\n",
    "\n",
    "# One-Hot Encoding of Categorical Features (remove possible 'ranking') \n",
    "one_hot_encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{column}_indexed\" for column in categorical_columns],\n",
    "    outputCols=[f\"{column}_encoded\" for column in categorical_columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create Feature Vectors\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"DEP_HOUR\", \"MONTH\", \"DISTANCE\"] + # Numerical Featured\n",
    "              [f\"{column}_encoded\" for column in categorical_columns], # Categorical Encoded Features\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f733bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Init Logistic Regression Model\n",
    "logistic_regression = LogisticRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"label\", \n",
    "    maxIter=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# ML-Pipeline inlcuding defined Preprocessing\n",
    "ml_pipeline = Pipeline(stages=string_indexers + [one_hot_encoder, feature_assembler, logistic_regression])\n",
    "\n",
    "# Split data in train and test\n",
    "training_data, test_data = ml_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Model\n",
    "trained_model = ml_pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Init Evaluator\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Creatre Predictions for Evaluation\n",
    "predictions = trained_model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "acc = accuracy_evaluator.evaluate(predictions)\n",
    "\n",
    "acc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datenanalyse-in-big-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
